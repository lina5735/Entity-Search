{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "196facb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from multiprocessing import Pool, Lock\n",
    "import shutil\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "from ast import literal_eval\n",
    "from file_operations import get_all_files, read_file, copy_file, process_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b971192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## download fastText model\n",
    "# import gensim.downloader as api\n",
    "# model = api.load('fasttext-wiki-news-subwords-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e78301b",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"../data/cnn_dailymail/data\"\n",
    "daily_path, cnn_path = \"/dailymail/stories/\", \"/cnn/stories/\"\n",
    "daily_docs = get_all_files(root + daily_path)\n",
    "cnn_docs = get_all_files(root + cnn_path)\n",
    "# file_paths = [f for f in os.listdir(root+\"/subset\") if f.endswith(\"story\")]        \n",
    "# file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3db1ee23-8f7e-48bd-83ae-3a4e48c0cf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = \"../data/tf_idf/full_text/\"\n",
    "entity = \"../data/entity/\"\n",
    "tfidf = \"../data/tf_idf/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ef319d-fda0-4009-88d2-51dc8fd38eda",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Vectorize Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3467103a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file_paths):\n",
    "    \n",
    "    docs2vec_path = root+\"/cnn_docs.csv\"\n",
    "\n",
    "    lock = Lock()\n",
    "\n",
    "    file = open(docs2vec_path, 'w', newline='')\n",
    "    csv_writer = csv.writer(file)\n",
    "    \n",
    "    with Pool(processes=8) as pool:\n",
    "        results = pool.imap(read_file, file_paths)\n",
    "\n",
    "        # Process the results as they become available\n",
    "        for ID, document in results:\n",
    "            # Do something with the content (process each file separately)\n",
    "            vectors = []\n",
    "            if not document:\n",
    "                continue\n",
    "            \n",
    "            # FastText word vectorization\n",
    "            for token in document:\n",
    "                if token in model:\n",
    "                    vectors.append(model[token])\n",
    "                    \n",
    "            # Averaging all words\n",
    "            vector = np.mean(vectors, axis=0) \n",
    "            ids = ID.split(\".\")[0]\n",
    "            \n",
    "            row = [ids] + list(vector)\n",
    "            \n",
    "            lock.acquire()\n",
    "            csv_writer.writerow(row) \n",
    "            lock.release()\n",
    "            \n",
    "    file.close()\n",
    "            \n",
    "    \n",
    "    \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15bf7e75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# process_file(daily_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "161ea55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_file(cnn_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bd45cc-c1c3-4c7c-b031-3d8c52bce445",
   "metadata": {},
   "source": [
    "## Copy Label File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2ef9e84-4f98-4e24-b069-b0f185fc844a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv(root+\"label.csv\")\n",
    "label_ids = labels.loc[:,\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e20b672b-9f01-4c2a-9661-a8e9cb17dec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_path_from_orgin(ids):\n",
    "    full_path = []\n",
    "    for file_name in ids:\n",
    "        if  not file_name.endswith(\"story\"):\n",
    "            file_name += \".story\"\n",
    "        if file_name in daily_docs:\n",
    "            full_path.append(os.path.join(root+daily_path, file_name))\n",
    "        else:\n",
    "            full_path.append(os.path.join(root+cnn_path, file_name))\n",
    "            \n",
    "    return full_path\n",
    "\n",
    "def join_path_to_dest(ids, dest):\n",
    "    full_path = []\n",
    "    for file_name in ids:\n",
    "        if  not file_name.endswith(\"story\"):\n",
    "            file_name += \".story\"\n",
    "        full_path.append(os.path.join(dest, file_name))\n",
    "    return full_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e409c07c-ff48-4060-8b44-05dffbe92f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_file_docs(label_ids, destination):\n",
    "    source_files = join_path_from_orgin(label_ids)\n",
    "    destination_files = join_path_to_dest(label_ids, destination)\n",
    "    \n",
    "    file_pairs = zip(source_files, destination_files)\n",
    "    \n",
    "    with Pool(processes=8) as pool:\n",
    "        pool.map(copy_file, file_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "58d536ff-a464-41e2-90c3-2d2fa3858351",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_file_docs(label_ids, \"../data/package_performance/manual_label/label_story/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1019971-abb6-4360-b2f7-275adce5b00f",
   "metadata": {},
   "source": [
    "## Compute vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa642fee-6863-45a2-aec0-c4fd4319c3e8",
   "metadata": {},
   "source": [
    "#### 1. full text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "663c0b65-2ef7-4de2-89d4-c77385e801cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_vocal(result, file_paths):\n",
    "\n",
    "    # lock = Lock()\n",
    "\n",
    "    with Pool(processes=8) as pool:\n",
    "        results = pool.imap(read_file, file_paths)\n",
    "\n",
    "        # Process the results as they become available\n",
    "        for ID, document in results:\n",
    "            # Do something with the content (process each file separately)\n",
    "            \n",
    "            if not document:\n",
    "                continue\n",
    "            \n",
    "            for token in document:\n",
    "                result.add(token)\n",
    "                    \n",
    "            # lock.acquire()\n",
    "            # lock.release()\n",
    "            \n",
    "    return result\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b2ad0f1-7cde-4fb3-8e59-6ea873261f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_vocabulary(vocal_set, destination):\n",
    "    vocal_set.discard(\"\")\n",
    "    \n",
    "    set_as_list = list(vocal_set)\n",
    "\n",
    "    # Write the set (list) to a JSON file\n",
    "    with open(destination, 'w', encoding='utf-8') as file:\n",
    "        json.dump(set_as_list, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a3e43e38-c64e-4aae-bc00-6b90d567ed30",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocal_set = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "802985c4-6a94-4b85-9b2e-0391950a007b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocal_set = compute_vocal(vocal_set, cnn_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f725cc99-d560-4efb-af93-8788dfd79fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocal_set = compute_vocal(vocal_set, daily_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd2a55b-7715-4ef3-8a64-408b1939d5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_vocabulary(vocal_set, full_text+'/vocabulary.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77325f9-b0cf-4a65-86a3-1214a3c0f6e4",
   "metadata": {},
   "source": [
    "#### 2. entity file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6580a452-7a53-4ba1-ac67-a8e2f7fb09f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entity_vocal(result, file_path, col=1):\n",
    "\n",
    "    entity_df = pd.read_excel(file_path)\n",
    "        \n",
    "    for s in entity_df.iloc[:,col]:\n",
    "        if s and len(s) > 0:\n",
    "            result.update(process_token(s))   \n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "891663cd-0941-4dc7-984e-1c81a75212d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocal_location = set()\n",
    "vocal_location = compute_entity_vocal(vocal_location,entity+\"cnn_location.xlsx\")\n",
    "vocal_location = compute_entity_vocal(vocal_location,entity+\"daily_loc.xlsx\")\n",
    "store_vocabulary(vocal_location, tfidf+\"location/vocabulary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e3245b3c-42f9-42ae-8021-f3f750d943fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocal_date = set()\n",
    "vocal_date = compute_entity_vocal(vocal_date,entity+\"cnn_date.xlsx\")\n",
    "vocal_date = compute_entity_vocal(vocal_date,entity+\"daily_date.xlsx\")\n",
    "\n",
    "store_vocabulary(vocal_date, tfidf+\"date/vocabulary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be4a6ea-f78a-4c14-a8c3-5dda7e86e464",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocal_people = set()\n",
    "vocal_people = compute_entity_vocal(vocal_people,entity+\"cnn_people.xlsx\")\n",
    "vocal_people = compute_entity_vocal(vocal_people,entity+\"daily_people_org.xlsx\")\n",
    "\n",
    "store_vocabulary(vocal_people, tfidf+\"people/vocabulary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d463bce-34d4-467d-b34a-bb80edaa8207",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocal_org = set()\n",
    "vocal_org = compute_entity_vocal(vocal_org,entity+\"cnn_organization.xlsx\")\n",
    "vocal_org = compute_entity_vocal(vocal_org,entity+\"daily_people_org.xlsx\", 2)\n",
    "\n",
    "store_vocabulary(vocal_org, tfidf+\"organization/vocabulary.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b750c47-fb07-4519-b146-67c324bce82a",
   "metadata": {},
   "source": [
    "## Compute IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbee450-aff2-4329-8955-718018c05fe1",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1. full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf10e411-d1f5-45a0-a88b-db5803e67ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(full_text+'/vocabulary.json', 'r') as file:\n",
    "    vocabulary = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0224743d-bafa-48f6-9c66-f84c1be09669",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_df(df_dict, file_paths):\n",
    "    \n",
    "    with Pool(processes=8) as pool:\n",
    "        results = pool.imap(read_file, file_paths)\n",
    "\n",
    "        # Process the results as they become available\n",
    "        for ID, document in results:\n",
    "            # Do something with the content (process each file separately)\n",
    "            \n",
    "            if not document:\n",
    "                continue\n",
    "                \n",
    "            # remove duplicated tokens\n",
    "            token_set = set(document)\n",
    "            \n",
    "            for token in token_set:\n",
    "                df_dict[token] += 1\n",
    "                \n",
    "    return df_dict\n",
    "                \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7673cb7-4c76-48fb-b090-c66d52d37e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_idf(idf):\n",
    "    N = len(daily_docs) + len(cnn_docs)\n",
    "    \n",
    "    for k,v in idf.items():\n",
    "        idf[k] = np.log(N/v)\n",
    "        \n",
    "    return idf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a114401c-5e68-417e-9f4a-44e98b344373",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = {element: 0 for element in vocabulary}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "528b457f-c60b-4270-9ef1-c29131e5c512",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = compute_df(idf, daily_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc745120-8b5a-48ae-bbc0-fba19246a54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = compute_df(idf, cnn_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ca2fbd9-9cca-4253-94b9-d0cc08535622",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = compute_idf(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4bce3241-2820-4f78-904a-40734099a17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the dictionary to a JSON file\n",
    "with open(full_text+'/idf.json', 'w') as file:\n",
    "    json.dump(idf, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74635268-e7d4-4a99-9718-268a1b4cd086",
   "metadata": {},
   "source": [
    "#### 2. entity file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b449bf6e-3e4e-4b12-9853-e766eef9fa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entity_df(df_dict, file_path, col=1):\n",
    "    entity_df = pd.read_excel(file_path)\n",
    "        \n",
    "    for s in entity_df.iloc[:,col]:\n",
    "        if s and len(s) > 0:\n",
    "            tokens = process_token(s)\n",
    "            token_set = set(tokens)\n",
    "            \n",
    "        # print(s, token_set)\n",
    "        for token in token_set:\n",
    "            df_dict[token] += 1\n",
    "            \n",
    "    return df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "141ac122-c4e7-4645-9b90-e16f1b44271b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(tfidf+\"date/vocabulary.json\", 'r') as file:\n",
    "    vocabulary = json.load(file)\n",
    "\n",
    "date_idf = {element: 0 for element in vocabulary}\n",
    "date_idf = compute_entity_df(date_idf, entity+\"cnn_date.xlsx\")\n",
    "date_idf = compute_entity_df(date_idf, entity+\"daily_date.xlsx\")\n",
    "\n",
    "date_idf = compute_idf(date_idf)\n",
    "with open(tfidf+'date/idf.json', 'w') as file:\n",
    "    json.dump(date_idf, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ae2818f-0f87-46a5-a0b1-85dd743d84c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(tfidf+\"location/vocabulary.json\", 'r') as file:\n",
    "    vocabulary = json.load(file)\n",
    "\n",
    "loc_idf = {element: 0 for element in vocabulary}\n",
    "loc_idf = compute_entity_df(loc_idf, entity+\"cnn_location.xlsx\")\n",
    "loc_idf = compute_entity_df(loc_idf, entity+\"daily_loc.xlsx\")\n",
    "\n",
    "loc_idf = compute_idf(loc_idf)\n",
    "with open(tfidf+'location/idf.json', 'w') as file:\n",
    "    json.dump(loc_idf, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6655db9-cd10-47a5-8d23-804f4b79c3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(tfidf+\"people/vocabulary.json\", 'r') as file:\n",
    "    vocabulary = json.load(file)\n",
    "\n",
    "people_idf = {element: 0 for element in vocabulary}\n",
    "people_idf = compute_entity_df(people_idf, entity+\"cnn_people.xlsx\")\n",
    "people_idf = compute_entity_df(people_idf, entity+\"daily_people_org.xlsx\")\n",
    "\n",
    "people_idf = compute_idf(people_idf)\n",
    "with open(tfidf+'people/idf.json', 'w') as file:\n",
    "    json.dump(people_idf, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8e246716-4411-4424-9f97-ce5a19d7f40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(tfidf+\"organization/vocabulary.json\", 'r') as file:\n",
    "    vocabulary = json.load(file)\n",
    "\n",
    "org_idf = {element: 0 for element in vocabulary}\n",
    "org_idf = compute_entity_df(org_idf, entity+\"cnn_organization.xlsx\")\n",
    "org_idf = compute_entity_df(org_idf, entity+\"daily_people_org.xlsx\", 2)\n",
    "\n",
    "org_idf = compute_idf(org_idf)\n",
    "with open(tfidf+'organization/idf.json', 'w') as file:\n",
    "    json.dump(org_idf, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
